defaults:
  # seed for random number generator
  seed: 0
  # training configurations
  train_cfgs:
    # device to use for training, options: cpu, cuda, cuda:0, cuda:0,1, etc.
    device: cpu
    # number of threads for torch
    torch_threads: 16
    # number of vectorized environments
    vector_env_nums: 1
    # number of parallel agent, similar to a3c
    parallel: 1
    # total number of steps to train
    total_steps: 10000000

  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 20000
    # number of iterations to update the policy
    update_iters: 10
    # batch size for each iteration
    batch_size: 128
    # target kl divergence
    target_kl: 0.01
    # entropy coefficient
    entropy_coef: 0.0
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: False
    # normalize observation
    obs_normalize: True
    # early stop when kl divergence is bigger than target kl
    kl_early_stop: False
    # use max gradient norm
    use_max_grad_norm: True
    # max gradient norm
    max_grad_norm: 40.0
    # use critic norm
    use_critic_norm: True
    # critic norm coefficient
    critic_norm_coef: 0.001
    # reward discount factor
    gamma: 0.99
    # cost discount factor
    cost_gamma: 0.99
    # lambda for gae
    lam: 0.95
    # lambda for cost gae
    lam_c: 0.95
    # advantage estimation method, options: gae, retrace
    adv_estimation_method: gae
    adv_estimation_method_safety: gae_safety
    # standardize reward advantage
    standardized_rew_adv: True
    # standardize cost advantage
    standardized_cost_adv: True
    # The type of advantage surrogate to use (in algo's method _compute_adv_surrogate)
    # penalty coefficient
    penalty_coef: 0.0
    # use cost
    use_cost: True
    # Damping value for conjugate gradient
    cg_damping: 0.1
    # Number of conjugate gradient iterations
    cg_iters: 15
    # Subsampled observation
    fvp_obs: None
    # The sub-sampling rate of the observation
    fvp_sample_freq: 1
  # logger configurations
  logger_cfgs:
    # use wandb for logging
    use_wandb: True
    # wandb project name
    wandb_project: omnisafe
    # use tensorboard for logging
    use_tensorboard: True
    # save model frequency
    save_model_freq: 100
    # save logger path
    log_dir: "./runs"
    # save model path
    window_lens: 100
  # model configurations
  model_cfgs:
    # weight initialization mode
    weight_initialization_mode: "kaiming_uniform"
    # actor type, options: gaussian, gaussian_learning
    actor_type: gaussian_learning
    # linear learning rate decay
    linear_lr_decay: False
    # exploration noise anneal
    exploration_noise_anneal: False
    # std upper bound, and lower bound
    std_range: [0.5, 0.1]
    # Update 05/15/24:
    # Maximum resampling for binary critic network
    max_resamples: 100
    # Action criterion: either take 'safest' action or 'first_safe'
    action_criterion: safest
    # How to compute the actor_critic safety index (to be used in advantage estimation)
    safety_index_criterion: avg
    # equality vs inequality in the bellman operator for safety critic (this does filtering on the mini-batch)
    operator: inequality
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [64, 64]
      # activation function
      activation: tanh
      # out_activation: tanh
      # learning rate
      lr: ~
    critic:
      # hidden layer sizes
      hidden_sizes: [64, 64]
      # activation function
      activation: tanh
      # learning rate
      lr: 0.001
    # The cost critic
    cost_critic:
      num_critics: 1
      hidden_sizes: [256, 256]
      activation: tanh
      lr: 0.001
    # The binary critic
    binary_critic:
      num_critics: 1
      hidden_sizes: [ 256, 256 ]
      activation: tanh
      lr: 0.001
      # The axiomatic dataset
      axiomatic_data:
        o: 10
        a: 5
        epochs: 100
      # 05/16/24: Optimistic init - train with these many (random) samples as 'safe'
      init_samples: 20_000

  # lagrangian configurations
  lagrange_cfgs:
    # Tolerance of constraint violation
    cost_limit: 25.0
    # Initial value of lagrangian multiplier
    lagrangian_multiplier_init: 0.001
    # Learning rate of lagrangian multiplier
    lambda_lr: 0.035
    # Type of lagrangian optimizer
    lambda_optimizer: "Adam"
